---
layout: post
title: word2vec 概要
date: 2018-5-12 
tags: word2vec
---

# word2vec概要

转自https://www.jianshu.com/p/bca4e7bfb86d



### 摘要

1.  google在2013年推出的一个NLP工具
2.  特点
    1.  将词**向量化**, 这样就可以定量的去度量词与词之间的关系, 挖掘词之间的联系
    2.  有距离性质, 向量空间上的**相似度**可以用来表示文本语义上的相似度
    3.  保持了**上下文信息**, 词语的信息更加地丰富
    4.  为文本数据寻求更加**深层次的特征表示**
    5.  有计算性质: 美国 \+ 波士顿 \- 伦敦 = 英国
3.  应用, **序列数据 \+ 局部强关联**
    1.  聚类, 找同义词, 词性分析
    2.  文本序列: **近邻强关联**, 可通过上下文预测目标词(选词填空)
    3.  社交网络: 随机游走生成**序列**, 然后使用word2vec训练每个节点的向量.
    4.  推荐系统, 广告(APP下载**序列**: word2vec + similarity = aggr to )

### 词向量基础

1.  one-hot representation

    1.  稀疏, 离散
    2.  维度灾难
2.  distribution representation

    1.  稠密, 连续

    2.  通过**训练**, 将词映射到一个较小的空间上

    3.  训练方法: NN(3 layers, input, hidder, output(softmax, 使用softmax选择概率最大的神经元))

![](https://upload-images.jianshu.io/upload_images/1629944-5f354d4b7a1198f6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/468)

### cbow vs skip-gram

cbow和skip-gram只是两种**标记方式**而已, 将词向量的学习过程转化为有监督学习

![](https://upload-images.jianshu.io/upload_images/1629944-740282092188213a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/694)

| CBOW                   | Skip-gram            |
| ---------------------- | -------------------- |
| 给定上下文预测词语     | 给定词语预测上下文   |
| 短文本+样本数多        | 长文本+样本数小      |
| 速度更快, 高频词更准确 | 对稀有词有较好的表示 |

###  hierarchical softmax vs negative sampling

其中, NN训练是非常慢的(需要计算所有词的softmax概率, 然后找最大的概率, 计算复杂度高)

因此有两种改进算法

*   hierarchical softmax(base huffman tree)
    *   改进
        *   输入层到隐藏层的映射: 直接求和取均值
        *   隐藏层到softmax层的映射: 使用Huffman tree替代
    *   训练更快, 对**高频词**支持较好
*   negative sampling
    *   改进: **负样本抽样降**低计算(有点类似mini-bath gradient descent的意思)
    *   训练较慢, 对**低频词**支持较好(解决了huffman树中低频词深度高的问题)

### gensim with word2vec

```
gensim.models.word2vec.Word2Vec(sentences=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=())

```

gensim默认的是cbow + negative sampling

size选择最多300就好了

---

参考:

[word2vec原理](https://link.jianshu.com?t=http%3A%2F%2Fwww.cnblogs.com%2Fpinard%2Fp%2F7160330.html)

[word2vec前世今生](https://link.jianshu.com?t=http%3A%2F%2Fwww.cnblogs.com%2Filoveai%2Fp%2Fword2vec.html)

[word2vec有什么应用？](https://link.jianshu.com?t=https%3A%2F%2Fwww.zhihu.com%2Fquestion%2F25269336)

[https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures](https://link.jianshu.com?t=https%3A%2F%2Fwww.quora.com%2FWhat-are-the-continuous-bag-of-words-and-skip-gram-architectures)

[https://www.quora.com/How-does-word2vec-work-Can-someone-walk-through-a-specific-example](https://link.jianshu.com?t=https%3A%2F%2Fwww.quora.com%2FHow-does-word2vec-work-Can-someone-walk-through-a-specific-example)

[https://stackoverflow.com/questions/39224236/word2vec-cbow-skip-gram-performance-wrt-training-dataset-size](https://link.jianshu.com?t=https%3A%2F%2Fstackoverflow.com%2Fquestions%2F39224236%2Fword2vec-cbow-skip-gram-performance-wrt-training-dataset-size)

[https://stats.stackexchange.com/questions/180076/why-is-hierarchical-softmax-better-for-infrequent-words-while-negative-sampling](https://link.jianshu.com?t=https%3A%2F%2Fstats.stackexchange.com%2Fquestions%2F180076%2Fwhy-is-hierarchical-softmax-better-for-infrequent-words-while-negative-sampling)

[https://stackoverflow.com/questions/26569299/word2vec-number-of-dimensions](https://link.jianshu.com?t=https%3A%2F%2Fstackoverflow.com%2Fquestions%2F26569299%2Fword2vec-number-of-dimensions)